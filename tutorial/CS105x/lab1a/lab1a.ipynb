{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Tutorial: Learning Apache Spark\n",
    "## Part 1: Basic notebook usage and Python integration\n",
    "### (1a) Notebook usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a python cell. You can run normal Python code here.\n",
    "print('hello world')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is another Python cell.\n",
    "x = 42\n",
    "if x > 40:\n",
    "    print('x is {0}. It is larger than 40.'.format(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1b) Notebook state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell relies on x being defined in the previous cell.\n",
    "# If we didn't run the previous cell, then this code would fail.\n",
    "print(x * 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1c) Library imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the regular expresion library.\n",
    "import re\n",
    "m = re.search('(?<=abc)def', 'abcdef')\n",
    "m.group(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the datetime library\n",
    "import datetime\n",
    "print('This was last run on : {0}'.format(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: An introduction to using Apache Spark with the PySpark SQL API running in a notebook\n",
    "### Spark Context\n",
    "In Spark, communication occurs between a driver and executors. The driver has Spark jobs that it needs to run and these jobs are split into tasks that are submitted to the executors for completion. The results from these tasks are delivered back to the driver.\n",
    "\n",
    "In part 1, we saw that normal Python code can be executed via cells. When using Databricks this code gets executed in the Spark driver's Java Virtual Machine (JVM) and not in an executor's JVM, and when using an Jupyter notebook it is executed within the kernel associated with the notebook. Since no Spark functionality is actually being used, no tasks are launched on the executors.\n",
    "\n",
    "In order to use Spark and its DataFrame API we will need to use a SQLContext. When running Spark, you start a new Spark application by creating a SparkContext. You can then create a SQLContext from the SparkContext. When the SparkContext is created, it asks the master for some cores to use to do work. The master sets these cores aside just for you; they won't be used for other applications. When using Databricks, both a SparkContext and a SQLContext are created for you automatically. sc is your SparkContext, and sqlContext is your SQLContext."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2a) SparkContext type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the type of the spark sqlContext\n",
    "type(sqlContext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2b) SparkContext attributes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List sqlContext's attributes\n",
    "dir(sqlContext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2c) Geeting help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(sqlContext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get what version of spark we are using\n",
    "sc.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Help can be used on any Python object\n",
    "help(map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Using DataFrames and chaining together transformations and actions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3a) Create a Python collection of 10,000 people\n",
    "We will use a third-party Python testing library called fake-factory to create a collection of fake person records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a fake name\n",
    "from faker import Factory\n",
    "fake = Factory.create()\n",
    "fake.seed(4321)\n",
    "fake.name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Each entry consists of last_name, first_name, ssn, job, and age (at least 1)\n",
    "from pyspark.sql import Row\n",
    "def fake_entry():\n",
    "    name = fake.name().split()\n",
    "    return Row(name[1], name[0], fake.ssn(), fake.job(), abs(2017-fake.date_time().year)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A helper function to call a function repeatedly\n",
    "def repeat(times, func, *args, **kwargs):\n",
    "    for _ in xrange(times):\n",
    "        yield func(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate fake data of 10,000 people\n",
    "data = list(repeat(10000, fake_entry))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print one entry\n",
    "data[0][0],data[0][1],data[0][2],data[0][3],data[0][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of entries\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3b) Distributed data and using a collection to create a DataFrame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a DataFrame in Spark\n",
    "dataDF = sqlContext.createDataFrame(data,('last_name', 'first_name','ssn','occupation','age'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the type of the new DataFrame \n",
    "print('type of dataDF: {0}'.format(type(dataDF)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the DataFrame's schema\n",
    "dataDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Register the new DataFrame as a named table\n",
    "sqlContext.registerDataFrameAsTable(dataDF, 'dataframe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display 20 lines of the new table\n",
    "sqlContext.sql(\"select * from dataframe\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how many partitions the DataFrame will be split into.\n",
    "dataDF.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check operations about DataFrame\n",
    "newDF = dataDF.distinct().select('*')\n",
    "newDF.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3c) Subtract one from each value using `select`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Subtract 1 from age of each entry using select transformation. A new column will be created by subtract 1 from 'age'\n",
    "# column and it will be renamed as 'age'.\n",
    "subDF = dataDF.select('last_name', 'first_name', 'ssn', 'occupation', (dataDF.age-1).alias('age'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Query plan\n",
    "subDF.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3d) Use `collect` to view results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using collect to gather data from multiple partitions\n",
    "results = subDF.collect()\n",
    "# Because the gathered data may be too large to display, using show() to display a small amount of data\n",
    "# print(results)\n",
    "subDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3e) Use `count` to get total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataDF.count())\n",
    "print(subDF.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3f) Apply transformation `filter` and view results with `collect`\n",
    "Each task makes a new partition with entries from the original partition that have an \"age\" column value less than 10. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "subDF.age.cast(IntegerType())\n",
    "filteredDF = subDF.filter(subDF.age < 10)\n",
    "filteredDF.show(truncate=False)\n",
    "filteredDF.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Python Lambda functions and User Defined Functions\n",
    "We can define a lambda function and then register it as a Spark *User Defined Function* (UDF) to filter data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import BooleanType\n",
    "from pyspark.sql.functions import udf\n",
    "less_ten = udf(lambda s: s<10, BooleanType())\n",
    "lambdaDF = subDF.filter(less_ten(subDF.age))\n",
    "lambdaDF.show()\n",
    "lambdaDF.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Additional DataFrame actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  `first()` & `take()`\n",
    "We can look at the first a few entries to get rough idea about the data. *first()* returns the first entry and *take(n)* returns the first n entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"first: {0}\\n\".format(filteredDF.first()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Four of them: {0}\\n\".format(filteredDF.take(4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Additional DataFrame transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6a) `orderBy()`\n",
    "`orderBy()` can be used to sort a DataFrame by one or more columns, producing a new DataFrame.\n",
    "To access a `Column` object, we have two notations on the DataFrame:\n",
    "* Pandas-style notation: `filteredDF.age`\n",
    "* Subscript notation: `filteredDF['age']`\n",
    "However, Pandas-style may have side effects. Using Subscript notation is always safe. Both syntaxes return a `Column`, which may have additional methods such as `desc()` and `asc()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by age in ascending order; returns a new DataFrame\n",
    "dataDF.orderBy(dataDF.age).show(n=5)\n",
    "\n",
    "# Sort by last name in descending order\n",
    "dataDF.orderBy(dataDF.last_name.desc()).show(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (6b) `distinct()` and `dropDuplicates()`\n",
    "`distinct()` filters out duplicate rows, and it will consider all columns. `dropDuplicates()` is similar as `distinct()`, except that it allows to define specific column to compare."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total number of original DataFrame and DataFrame with distinct entries. Because the DataFrame here is generated randomly, it is unlikely to have duplicated entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataDF.count())\n",
    "print(dataDF.distinct().count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a dummy DataFrame to show the effect of `distinct()` and `dropDuplicates()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A dummy DataFrame with duplicated entries.\n",
    "tempDF = sqlContext.createDataFrame([(\"Joe\", 1), (\"Joe\", 1), (\"Anna\", 15), (\"Anna\", 12), (\"Ravi\", 5)], ('name', 'score'))\n",
    "# Original DataFrame\n",
    "tempDF.show()\n",
    "# Distinct DataFrame\n",
    "tempDF.distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `dropDuplicates()` to remove duplicated entries of specific columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tempDF.count())\n",
    "print(tempDF.dropDuplicates(['name']).count())\n",
    "tempDF.dropDuplicates(['name']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6c) `drop()`\n",
    "We can use `drop()` to drop some columns from a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDF.drop('occupation').drop('age').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6d) `groupBy()`\n",
    "`groupBy()` allows to perform aggregation on a DataFrame. It returns a special GroupedData object that can apply various aggregation operations such as `count()`, `sum()`, `max()`, and `avg()`. These aggregation functions typically create a new Column and return a new DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count total number of people of each occupation.\n",
    "dataDF.groupBy('occupation').count().show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute average age.\n",
    "dataDF.groupBy().avg('age').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute max and min ages.\n",
    "print('Maximum age: {0}'.format(dataDF.groupBy().max('age').first()[0]))\n",
    "print('Minimum age: {0}'.format(dataDF.groupBy().min('age').first()[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6e) `sample()`\n",
    "`sample()` returns a random sample of DataFrame. `withReplacement` argument can specify whether sample with or without replacement. `fraction` parameter specifies the fraction elements to be returned. (`fraction=0.2` returns 20% of the elements in the DataFrame.) `seed` is used to set the seed for RNG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampledDF = dataDF.sample(withReplacement=False, fraction=0.10)\n",
    "print(sampledDF.count())\n",
    "sampledDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataDF.sample(withReplacement=False,fraction=0.05).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Caching DataFrames and storage options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (7a) Caching DataFrames\n",
    "`cache()` can cache data in Spark. Caching can improve efficiency when you plan to use a DataFrame multiple times. However, an action on the DataFrame should be triggered before the caching will occur because of the lazy mechanism in Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cache the DataFrame\n",
    "filteredDF.cache()\n",
    "# Trigger an action\n",
    "print(filteredDF.count())\n",
    "# The DataFrame is indeed cached\n",
    "print(filteredDF.is_cached)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (7b) Unpersist and storage options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `unpersist()` to reclaim the memory used by previously cached DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filteredDF.unpersist()\n",
    "print(filteredDF.is_cached)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Debugging Spark applications and lazy evaluation\n",
    "### How Python is Executed in Spark\n",
    "Internally, Spark executes using a Java Virtual Machine (JVM). pySpark runs Python code in a JVM using Py4J. Py4J enables Python programs running in a Python interpreter to dynamically access Java objects in a Java Virtual Machine. Methods are called as if the Java objects resided in the Python interpreter and Java collections can be accessed through standard Python collection methods. Py4J also enables Java programs to call back Python objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (8a) Challenges with lazy evalutaion using transformations and actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `brokenTen` function has a mistake in `if` statement. However, due to the lazy evaluation in Spark, the code will not be actually executed until an *action* is called on the DataFrame. Notice that `fitler()` will not trigger execution of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def brokenTen(value):\n",
    "    ''' Check whether a variable is less than ten.\n",
    "    Input:\n",
    "        value: the value to be compared with ten\n",
    "    Output: \n",
    "        boolean: True if value < 10 else False\n",
    "    Note:\n",
    "        In if statement the variable val is undefined, which will throw an error.\n",
    "    \n",
    "    '''\n",
    "    if (val < 10):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "btUDF = udf(brokenTen)\n",
    "brokenDF = subDF.filter(btUDF(subDF.age)==True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When an action is called on the DataFrame, `brokenTen` function will be called and an error will occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "brokenDF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We may use lambda function to improve readability and conciseness\n",
    "myUDF = udf(lambda v: v < 10, BooleanType())\n",
    "subDF.filter(myUDF(subDF.age) == True).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (8d) Readability and code style\n",
    "To make coding style more readable, statement with multiple methods, transformations, and actions can be enclosed in parentheses and each method, transformation, and action on a separate line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "(dataDF\n",
    "    .filter(dataDF.age > 20)\n",
    "    .select(concat(dataDF.first_name, lit(' '), dataDF.last_name), dataDF.occupation)\n",
    "    .show(truncate=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
