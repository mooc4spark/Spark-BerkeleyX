{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Count Lab: Building a word count application\n",
    "This lab intends to cover techinques in Spark for a simple word count application. In this lab, we will calculate the most common words in the Complete Works of William Shakespeare retrieved from [Project Gutenberg](http://www.gutenberg.org/wiki/Main_Page). This could be scaled in larger applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Creating a base DataFrame and performing operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1a) Create a DataFrame\n",
    "We can generate a base DataFrame from a Python list of tuples and the `sqlContext.createDataFrame` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|    word|\n",
      "+--------+\n",
      "|     cat|\n",
      "|elephant|\n",
      "|     rat|\n",
      "|     rat|\n",
      "|     cat|\n",
      "+--------+\n",
      "\n",
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "root\n",
      " |-- word: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wordsDF = sqlContext.createDataFrame([('cat',), ('elephant',), ('rat',), ('rat',), ('cat', )], ['word'])\n",
    "wordsDF.show()\n",
    "print(type(wordsDF))\n",
    "wordsDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1b) Using DataFrame functions to add an 's'\n",
    "We can use DataFrame functions to manipulate elements in a column. For example, we can add an 's' to every word in our base DataFrame. We first use `concat` function to combine multiple string columns to a single string column. Notice that `lit` function is used to convert a string literal to a column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|     word|\n",
      "+---------+\n",
      "|     cats|\n",
      "|elephants|\n",
      "|     rats|\n",
      "|     rats|\n",
      "|     cats|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit, concat\n",
    "pluralDF = wordsDF.select(concat(wordsDF.word, lit('s')).alias('word'))\n",
    "pluralDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1c) Length of each word\n",
    "We can also use the `length` function to compute the number of characters in each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|length(word)|\n",
      "+------------+\n",
      "|           4|\n",
      "|           9|\n",
      "|           4|\n",
      "|           4|\n",
      "|           4|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import length\n",
    "pluralLengthsDF = pluralDF.select(length(pluralDF.word))\n",
    "pluralLengthsDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Counting with Spark SQL and DataFrames\n",
    "There are multiple ways to compute the number of times a particular element appears in a specified column. The naive way is call `collect` on all the elements and counts them in the driver program. This method performs slowly when the dataset is terabytes. An efficient way is to apply data parallel operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2a) Using `groupby` and `count`\n",
    "We can perform aggregation by `groupby` function and then use `count` on the GroupedData object to count the ocurrences in the groups. To count the number of each words, we apply `groupby` on the 'word' column and use count to get the numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|    word|count|\n",
      "+--------+-----+\n",
      "|     rat|    2|\n",
      "|     cat|    2|\n",
      "|elephant|    1|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wordCountsDF = (wordsDF\n",
    "                .groupBy(wordsDF.word)\n",
    "                .count()\n",
    ")\n",
    "wordCountsDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Finding unique words and a mean value\n",
    "### (3a) Unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "# Number of different words in our DataFrame\n",
    "uniqueWordsCount = wordCountsDF.count()\n",
    "print(uniqueWordsCount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3b) Means of groups using DataFrames\n",
    "We can find the mean number of occurrences of words in `wordCountDF`. Notice that nothing is passed in `groupBy`. This just makes a DataFrame to be a groupedData object so that aggregation functions can be applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.66666666667\n"
     ]
    }
   ],
   "source": [
    "# Average number of occurrence\n",
    "averageCount = wordCountsDF.groupBy().mean('count').head()[0]\n",
    "#wordCountsDF.groupBy().select(mean().alias(\"count\"))\n",
    "print(averageCount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Apply word count to a file "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4a) The `wordCount` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|    word|count|\n",
      "+--------+-----+\n",
      "|     rat|    2|\n",
      "|     cat|    2|\n",
      "|elephant|    1|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The wordCount function to compute number of each words in a DataFrame\n",
    "def wordCount(wordListDF):\n",
    "    assert(str(type(wordListDF)) == \"<class 'pyspark.sql.dataframe.DataFrame'>\")\n",
    "    assert('word' in wordListDF.columns)\n",
    "    return wordListDF.groupBy('word').count()\n",
    "wordCount(wordsDF).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4a) Capitalization and punctuation\n",
    "The real word documents generally contains captialized letters and punctuation. We can use regular expressions to convert uppercase letters to lowercase and remove all punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------+\n",
      "|sentence                                    |\n",
      "+--------------------------------------------+\n",
      "|Hi, you!                                    |\n",
      "| No under_score                             |\n",
      "| *      Remove punctuation and spaces   *   |\n",
      "+--------------------------------------------+\n",
      "\n",
      "+-----------------------------+\n",
      "|sentence                     |\n",
      "+-----------------------------+\n",
      "|hi you                       |\n",
      "|no underscore                |\n",
      "|remove punctuation and spaces|\n",
      "+-----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import regexp_replace, trim, col, lower\n",
    "\n",
    "# The removePunctuation function\n",
    "def removePunctuation(column):\n",
    "    return trim(regexp_replace(lower(column), '[^0-9a-z\\s]', '')).alias('sentence')\n",
    "\n",
    "# A testing DataFrame for removePunctuation\n",
    "sentenceDF = sqlContext.createDataFrame([('Hi, you!',),\n",
    "                                         (' No under_score',),\n",
    "                                         (' *      Remove punctuation and spaces   *   ',),\n",
    "                                        ],['sentence'])\n",
    "sentenceDF.show(truncate=False)\n",
    "(sentenceDF\n",
    " .select(removePunctuation(col('sentence')))\n",
    " .show(truncate=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4c) Load a text file\n",
    "We will apply the `wordCount` function on the Complete Works of William Shakespeare from Project Gutenberg. First, we load the file from local to HDFS.\n",
    "`hadoop fs -copyFromLocal /path/to/file/shakespeare.txt /target/path/on/cluster`\n",
    "Then we use `sqlContext.read.text()` method to load a text file to a DataFrame. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------+\n",
      "|sentence                                         |\n",
      "+-------------------------------------------------+\n",
      "|1609                                             |\n",
      "|                                                 |\n",
      "|the sonnets                                      |\n",
      "|                                                 |\n",
      "|by william shakespeare                           |\n",
      "|                                                 |\n",
      "|                                                 |\n",
      "|                                                 |\n",
      "|1                                                |\n",
      "|from fairest creatures we desire increase        |\n",
      "|that thereby beautys rose might never die        |\n",
      "|but as the riper should by time decease          |\n",
      "|his tender heir might bear his memory            |\n",
      "|but thou contracted to thine own bright eyes     |\n",
      "|feedst thy lights flame with selfsubstantial fuel|\n",
      "+-------------------------------------------------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fileName = 'shakespeare.txt'\n",
    "shakespeareDF = sqlContext.read.text(fileName).select(removePunctuation(col('value')))\n",
    "shakespeareDF.show(15, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4d) Words from lines\n",
    "To apply the `wordCount` function, we have two extra steps,\n",
    "* split each sentence into a list of words by its spaces\n",
    "* filter out empty lines or words\n",
    "\n",
    "We use `split` and `explode` methods to accomplish these two tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|       word|\n",
      "+-----------+\n",
      "|       1609|\n",
      "|        the|\n",
      "|    sonnets|\n",
      "|         by|\n",
      "|    william|\n",
      "|shakespeare|\n",
      "|          1|\n",
      "|       from|\n",
      "|    fairest|\n",
      "|  creatures|\n",
      "|         we|\n",
      "|     desire|\n",
      "|   increase|\n",
      "|       that|\n",
      "|    thereby|\n",
      "|    beautys|\n",
      "|       rose|\n",
      "|      might|\n",
      "|      never|\n",
      "|        die|\n",
      "+-----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "882996\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split, explode\n",
    "shakeWordsDF = (shakespeareDF\n",
    "                .select(explode(split(shakespeareDF.sentence, ' ')).alias('word'))\n",
    "                .where(col('word') != ''))\n",
    "shakeWordsDF.show()\n",
    "shakeWordsDFCount = shakeWordsDF.count()\n",
    "print(shakeWordsDFCount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4e) Count the words\n",
    "Now we can use the `wordCount` function to perform word counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|word|count|\n",
      "+----+-----+\n",
      "|the |27361|\n",
      "|and |26028|\n",
      "|i   |20681|\n",
      "|to  |19150|\n",
      "|of  |17463|\n",
      "|a   |14593|\n",
      "|you |13615|\n",
      "|my  |12481|\n",
      "|in  |10956|\n",
      "|that|10890|\n",
      "|is  |9134 |\n",
      "|not |8497 |\n",
      "|with|7771 |\n",
      "|me  |7769 |\n",
      "|it  |7678 |\n",
      "|for |7558 |\n",
      "|his |6857 |\n",
      "|be  |6857 |\n",
      "|your|6655 |\n",
      "|this|6602 |\n",
      "+----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import desc\n",
    "topWordsAndCountsDF = wordCount(shakeWordsDF)\n",
    "# Display the most commonly used words\n",
    "topWordsAndCountsDF.orderBy(col('count').desc()).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|      word|count|\n",
      "+----------+-----+\n",
      "|       art|  915|\n",
      "|      some| 1337|\n",
      "|     those|  545|\n",
      "|     still|  552|\n",
      "|  painters|    1|\n",
      "|      hope|  354|\n",
      "|    travel|   33|\n",
      "|     cures|    8|\n",
      "|    ransom|   53|\n",
      "|     spoil|   25|\n",
      "|   tresses|    3|\n",
      "|       few|   64|\n",
      "| forgetful|    5|\n",
      "|    harder|   11|\n",
      "|  tripping|    6|\n",
      "| soundness|    1|\n",
      "|    waters|   27|\n",
      "|occidental|    1|\n",
      "|    marrow|    4|\n",
      "|    distil|    4|\n",
      "+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "topWordsAndCountsDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
